<kendo-pdf-export _ngcontent-uvf-c168="" papersize="A4" margin="2cm" id="div-book-content" class="col-12 col-sm-10 offset-0 div-book-content" uipath_custom_id="10"><div><app-page-content _ngcontent-uvf-c168="" _nghost-uvf-c166="" class="font-size-14 font-size-sm-20"><div _ngcontent-uvf-c166="" class="mainContent mt-4"><div _ngcontent-uvf-c166="" id="bookContent" class="bookContent"><div _ngcontent-uvf-c166="" id="content"><div class="sect1" style="abcpdf-tag-visible: true" id="AU_49f2da08-8f4b-4328-af19-28488c960cab" uri="ImagesUri_../download/5c180f28-f752-47db-98e8-8656c84b148d/images/">
      <h1 class="title">Bonnes pratiques de production </h1>
      <div class="sect2" id="refTitle0">
        <h2 class="title">1. Supervision<var style="display:none"> Supervision</var></h2>
        <div class="sect3" id="refTitle1">
          <h3 class="title">a. Approche "bétail plutôt qu’animal
domestique"</h3>
          <p class="defaut">Ce concept a déjà été évoqué plusieurs
fois dans les chapitres précédents sous le vocable
anglais consacré de <span class="italic">cattle versus pet</span>,
mais il est important de rappeler ici son impact sur l’évolution
de la supervision des instances de conteneurs. Le passage d’une
approche individuelle à une approche de gestion de la masse
a pour conséquence qu’au lieu de passer du temps à essayer
de remettre en route un service dans un conteneur, l’opération
prioritaire sera de supprimer ce conteneur et d’en relancer un dans
la même configuration. C’est seulement si l’erreur se reproduit
de manière régulière qu’il conviendra
d’enquêter.<var style="display:none"> cattle versus pet</var></p>
          <div class="note">
            <div class="remarkimg"><span class="icon-note"></span></div>
            <div class="divinline">
              <p class="remarque">Cette approche pourrait paraître
contreproductive, car elle peut être prise comme le fait
de masquer les problèmes et de chercher seulement une résolution
rapide, au mépris&nbsp;de la consommation de ressources
et de l’efficience à long terme. En pratique, il s’agit
d’un compromis d’efficacité, certains problèmes
informatiques étant éminemment compliqués à corriger,
alors que leurs effets de bord sont connus et maîtrisés.
Pour citer un exemple, la gestion des fuites de mémoire
sur des processus comme les serveurs web a longtemps fait l’objet
de correctifs sur le code, pour supprimer la racine des plus grosses fuites.
Mais au bout d’un moment, il est devenu de plus en plus complexe
de trouver les quelques fuites résiduelles, et le coût
de leur correction dépassait largement le gain. De plus,
ces serveurs devant intégrer des codes sources tiers qui étaient
parfois très loin d’avoir la même qualité,
les fuites de mémoire persistaient de toute manière&nbsp;lors
de l’utilisation intensive des fonctionnalités. La solution,
peu élégante mais efficace, a consisté à mettre
en place des fonctionnalités intégrées
de recyclage des processus qui, à intervalles réguliers
ou lorsque des seuils précis de consommation mémoire&nbsp;sont
atteints, vont purger complètement cette dernière
de façon à rétablir un comportement correct.
Des mécanismes sophistiqués permettent d’ailleurs
de réaliser ces purges sans que l’utilisateur ne le ressente.
Il en va donc de même que pour la plupart des bateaux où les
quelques fuites mineures sont compensées par une pompe
de cale qui rejette l’eau en continu. La coque n’est effectivement
pas 100&nbsp;% étanche, mais le bateau remplit
son office de transport, et c’est l’usage qui compte.</p>
            </div>
          </div>
          <p class="defaut">Contrairement à ce que sa désignation
de "bétail" pourrait laisser penser, le contenu applicatif
d’un conteneur doit être assez robuste et donc de haute qualité pour
fonctionner dans ce mode. Par exemple, un applicatif qui dans une
approche monolithique s’arrêtait sur une exception logicielle
et un message d’erreur s’il n’avait pas accès à une
base de données doit, dans des architectures de services
distribués, recommencer plusieurs fois avec un léger
délai, en ouvrant au besoin une nouvelle connexion. Puis,
dans le cas extrême où le problème est
toujours présent, il devra trouver un autre moyen d’opérer
(stocker la commande à exécuter plus tard mais
avec la même date de valeur, etc.), remonter un refus d’opération,
informer l’administrateur et, dans tous les cas, ne pas s’arrêter.
Bref, il s’agit d’une sérieuse étape de sophistication
logicielle à franchir par rapport aux approches traditionnelles
(et encore n’avons-nous pas parlé de la difficulté&nbsp;de
gérer la péremption de l’authentification pendant
le délai de réessai ni d’autres problématiques
de même ordre qui adviennent en production).</p>
          <p class="defaut">Pour revenir au premier chapitre, autant Docker
aide au découplage entre les couches applicatives et d’infrastructure,
autant il est nécessaire que les applications déployées
dessus aient également réalisé l’effort
de découplage des fonctionnalités et des applicatifs
associés pour ne pas que cet effort soit vain. L’administrateur
des applications en services hébergés dans des
conteneurs sera nécessairement aux avant-postes de cette
transformation des architectures, avec pour conséquences
pratiques des changements forts dans le maintien en condition opérationnelle
et la supervision de ces applications.&nbsp;De manière
très prosaïque, les machines n’étant
plus nommées avec des désignations représentatives
de leur contenu (voir plus haut), l’administrateur devra avoir d’autres
moyens de retrouver le contenu (travailler uniquement au niveau
des conteneurs, utiliser les métadonnées, maintenir
une cartographie, etc.).</p>
        </div>
        <div class="sect3" id="refTitle2">
          <h3 class="title">b. Outillage possible</h3>
          <p class="defaut">Les commandes <span class="courier11">docker
system df</span> et <span class="courier11">docker system prune</span>,
citées plus haut, peuvent être des outils intéressants
pour l’administrateur du cluster au niveau applicatif, lui permettant
de savoir combien une application utilise de ressources et éventuellement
de récupérer des espaces disques désormais inutiles.
De même, <span class="courier11">docker stats</span> est
certes un outil rudimentaire, mais qui conviendra tout à fait
pour des usages simples.<var style="display:none"> docker system</var></p>
          <p class="defaut">Dans la jungle des outils de monitoring pour
Docker qui est récemment apparue, il est très
difficile de faire un choix ou même simplement d’émettre
des recommandations sur tel ou tel outil par rapport à une
fonctionnalité attendue, et l’auteur se cantonnera donc à lister
quelques solutions possibles&nbsp;:<var style="display:none"> Monitoring</var></p>
          <div class="divliste1">
            <ul class="liste1">
              <li class="liste1">
                <p class="liste1">Google CAdvisor&nbsp;:
gratuit, très facile à utiliser, mais ne fait
pour l’instant pas grand-chose de plus que fournir une interface
graphique à <span class="courier11">docker stats</span>.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Sensu, avec son plug-in pour Docker&nbsp;:
open source, plus complet mais assez complexe à utiliser
car nécessite du paramétrage pas toujours évident.</p>
              </li>
              <li class="liste1">
                <p class="liste1">DataDog et Scout sont des solutions
payantes et en ligne qui se basent sur la présence d’un
agent dans les conteneurs pour remonter de l’information à un
serveur central qui permet de visualiser les agrégats,
indicateurs de consommation de ressources,&nbsp;etc.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Dynatrace ajoute un aspect plus
visuel et va au-delà du monitoring avec une approche&nbsp;de
pilotage de la montée en charge. Une de ses particularités
est de ne pas installer d’agent sur les conteneurs à surveiller.</p>
              </li>
              <li class="liste1">
                <p class="liste1">New Relic est une solution connue
et éprouvée, et qui est bien plus large que la
simple supervision de conteneurs Docker, car elle remonte également
des informations liées à Amazon Web Service et
consolide ces données.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Prometheus est un service open source
de monitoring, donc assez équivalent à DataDog&nbsp;et
Scout mais gratuit, contrairement à eux. Le serveur peut également être
installé en local. Grafana est souvent utilisé en
conjonction avec Prometheus pour l’affichage, ce dernier étant
plutôt orienté collecte des données.
CAdvisor possède&nbsp;une intégration native
de Prometheus pour cela.</p>
              </li>
              <li class="liste1">
                <p class="liste1">CoScale est une solution commerciale
dont le différenciateur est de s’intéresser principalement
aux applications en microservices elles-mêmes, la récupération
des métriques Docker étant une partie seulement
des données collectées sur la stack complète.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Et bien sûr la pile ELK
(ElasticSearch, LogStash, Kibana) dont la collecte peut être
paramétrée pour récupérer des
métriques des conteneurs par l’intermédiaire des
logs et donc fournir un suivi des consommations, erreurs applicatives,
etc.</p>
              </li>
            </ul>
          </div>
          <p class="defaut">À ce jour, aucune solution ne peut
se targuer d’une intégration native avec le reste de l’écosystème
Docker. Toutefois, la version 1.13 de Docker a annoncé dans
les fonctionnalités expérimentales une sortie
au format Prometheus de métriques en provenance du démon
Docker, en précisant que les versions futures compléteraient
fortement la liste pour l’instant restreinte de métriques remontées.
Cette marque d’intérêt&nbsp;peut être
un critère de sélection de ce logiciel par rapport à d’autres.</p>
        </div>
        <div class="sect3" id="refTitle3">
          <h3 class="title">c. Outillage des logs<var style="display:none"> Logs</var></h3>
          <p class="defaut">Une des approches les plus standards pour
contrer le problème cité plus haut de dispersion
des données est de centraliser la collecte de logs. Traditionnellement,
ceci est réalisé en utilisant deux outils&nbsp;:</p>
          <div class="divliste1">
            <ul class="liste1">
              <li class="liste1">
                <p class="liste1">L’outil de gestion
des logs standards de Linux. Docker recommandant fortement de loguer
sur la console pour avoir une façon simple et standard
de remonter des informations, <span class="courier11">syslog</span> est
naturellement adapté pour servir de relais à ces
flux d’information. L’option <span class="courier11">--log-driver=syslog</span> permet de
renvoyer ce flux du démon Docker vers le démon <span class="courier11">syslogd</span>. L’utilisateur bénéficie
alors de toutes les facilités associées à ce
mode de fonctionnement standard de Linux.<var style="display:none"> syslog</var></p>
              </li>
              <li class="liste1">
                <p class="liste1">La pile ELK déjà citée.
Encore une fois, il est possible de brancher le driver de log de
Docker sur une sortie dédiée, en l’occurrence
sur le format GELF. Ce format est pris en charge par Logstash et
permet donc de centraliser les logs dans ElasticSearch, pour une
exploitation graphique par des tableaux Kibana.</p>
              </li>
            </ul>
          </div>
          <p class="defaut">De nombreux autres drivers sont disponibles,
dont un pour le gestionnaire d’événements de Windows,
un pour <span class="courier11">fluentd</span>, un pour <span class="courier11">journald</span>, etc. Le driver par défaut
est <span class="courier11">json-file</span> et, avec le driver <span class="courier11">journald</span>, c’est le seul qui supporte
la commande <span class="courier11">docker logs</span>.</p>
          <p class="defaut">Dans tous les cas, le driver peut être
déterminé au niveau du conteneur ou alors au niveau
du démon, et il s’appliquera alors à l’ensemble
des conteneurs. Tous les détails des paramètres
optionnels pour chaque driver sont disponibles sur la documentation
officielle à l’adresse <a class="url" href="https://docs.docker.com/engine/admin/logging/overview/" target="_blank">https://docs.docker.com/engine/admin/logging/overview/</a></p>
          <div class="note">
            <div class="remarkimg"><span class="icon-note"></span></div>
            <div class="divinline">
              <p class="remarque">À noter que depuis la version
1.13, la commande <span class="courier11">docker service logs</span> permet
de centraliser les logs de toutes les instances d’un service dans
un Docker Swarm, ce qui évite énormément
de travail pour localiser et rapatrier les logs.</p>
            </div>
          </div>
          <p class="defaut">Évidemment, tous les outils utilisés
au niveau applicatif avant passage sous Docker peuvent tout à fait
continuer à être utilisés. Par exemple,
l’écriture de logs dans des fichiers peut être
réalisée sur des fichiers locaux, voire dans un conteneur
de type volume&nbsp;ou même par une redirection de
volume Docker sur un espace disque local au démon.</p>
        </div>
      </div>
      <div class="sect2" id="refTitle4">
        <h2 class="title">2. Bonnes pratiques logicielles</h2>
        <p class="defaut">Jusqu’à maintenant, nous avons cité de
nombreuses bonnes pratiques liées à Docker et à son
utilisation, mais il existe également de bonnes pratiques à suivre
sur les applicatifs qui seront fournis sous forme d’images Docker,
pour s’intégrer au mieux dans l’approche associée
et en tirer pleinement les bénéfices.</p>
        <div class="sect3" id="refTitle5">
          <h3 class="title">a. Importance de la normalisation des logs</h3>
          <p class="defaut">En lien avec le monitoring cité plus
haut, il est important de rentrer dans des systèmes de
gestion de logs unifiés permettant leur centralisation,
qui est l’approche privilégiée dans les architectures
distribuées (sous peine de perdre énormément
de temps, avant même de les analyser, à trouver
la bonne machine et le bon fichier de log). Ceci signifie bien sûr
que tous les applicatifs doivent fonctionner de manière
unifiée et, si possible, normalisée.</p>
          <p class="defaut">Fort heureusement, Docker propose déjà une
manière extrêmement simple et disponible dans
n’importe quel framework de développement (et même
quel que soit le système d’exploitation, d’ailleurs) pour
fournir des informations sur les processus conteneurisés, à savoir
les inscrire sur le flux de sortie de la console. Toutefois, ceci
ne correspond qu’au protocole pour véhiculer la donnée
et ne décrit en rien le format.</p>
          <p class="defaut">C’est sur ce point que l’effort de normalisation
doit être réalisé, en utilisant au maximum
les bonnes pratiques de l’industrie, que le lecteur retrouvera facilement
en se documentant&nbsp;sur les termes de "semantic logging"
ou "structured logging". Dans un premier temps, le seul fait de
produire des entrées de logs au format JSON plutôt
que dans une simple ligne de texte sans aucune structure permet
de grands pas en avant dans l’analyse et la valorisation des logs.</p>
        </div>
        <div class="sect3" id="refTitle6">
          <h3 class="title">b. API de statut<var style="display:none"> API</var><var style="display:none"> Statut</var></h3>
          <p class="defaut">Les services devenant de plus en plus disséminés
et étant par construction les plus autonomes&nbsp;possible
les uns des autres, il est nécessaire de pouvoir facilement
interroger un statut de fonctionnement de chacun de manière
automatisée. Une bonne pratique émergeant de l’industrie
consiste à équiper tous les services d’une API
supplémentaire de type statut et répondant aux
deux ordres ci-dessous&nbsp;:</p>
          <div class="divliste1">
            <ul class="liste1">
              <li class="liste1">
                <p class="liste1">Statut simple (typiquement
sur <span class="courier11">/status/shallow</span>)&nbsp;:
est-ce que le service est correctement démarré&nbsp;?
Le code HTTP 200 en retour montrera que c’est le cas, et tout autre
code montrera les problèmes d’accès au service.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Statut complet (typiquement sur <span class="courier11">/status/deep</span>)&nbsp;:
est-ce que le service est entièrement fonctionnel&nbsp;?
Encore une fois, les codes HTTP peuvent servir de retour des différents états
de ce statut, mais le body sera parfois nécessaire pour
donner les informations&nbsp;supplémentaires liées à cette
seconde méthode. En particulier, le statut complet vérifiera
si toutes les dépendances du service sont elles-mêmes
prêtes. Une pratique standard consiste à lancer un
miniscénario métier de référence
sur lequel&nbsp;on prendra soin de supprimer toute action persistante
après coup pour ne pas risquer d’effet de bord (par exemple,
pour une application de gestion de stock, créer un article,
ajouter une pièce, puis supprimer immédiatement
la référence). Une autre approche équivalente
est d’englober le scénario dans une transaction qui sera
annulée à la fin de son exécution ou
dès la première erreur rencontrée.</p>
              </li>
            </ul>
          </div>
          <p class="defaut">Idéalement, le système de
monitoring choisi sera capable d’appeler ces URL, de façon à déterminer
l’état de santé d’un service en profondeur, sans
se contenter de tester uniquement que l’URL de base du service est
accessible, ce qui ne prouve pas grand-chose.</p>
          <div class="note">
            <div class="remarkimg"><span class="icon-note"></span></div>
            <div class="divinline">
              <p class="remarque">Le mot-clé <span class="courier11">HEALTHCHECK</span> dans un <span class="courier11">Dockerfile</span>, présenté dans
un chapitre précédent, permet également
de mettre en place un système de validation de l’état
de santé d’un service.</p>
            </div>
          </div>
        </div>
        <div class="sect3" id="refTitle7">
          <h3 class="title">c. Retry policies et circuit breakers<var style="display:none"> Retry Policies</var><var style="display:none"> Circuit breakers</var></h3>
          <p class="defaut">Nous avons déjà évoqué le
besoin de changement dans le comportement des services par rapport à une
architecture monolithique. Une insistance particulière
avait été portée&nbsp;sur la nécessité&nbsp;de
garder une robustesse d’envoi de messages et de remontée
d’erreurs dans un mode distribué et avec une dépendance à un
réseau. Aucun réseau ne pouvant assurer&nbsp;une
garantie de livraison des paquets, même si certains sont
plus robustes que d’autres, il y a une incertitude fondamentale
sur l’appel d’un service par un autre.</p>
          <p class="defaut">Pour cela, il est nécessaire de mettre
en place des systèmes de rejeu des ordres lorsqu’ils ne
passent pas. En règle générale, un léger
délai est ménagé entre les essais et
un nombre maximum d’essais ou un temps maximum avant arrêt sont également
mis en place. L’ensemble de ces paramètres constitue ce
qu’on appelle la "retry policy" (littéralement "politique
de rejeu"). Mais pour aller au bout de la complexité du
système, il est également nécessaire
dans certains cas de pousser encore plus loin ces comportements
logiciels&nbsp;:</p>
          <div class="divliste1">
            <ul class="liste1">
              <li class="liste1">
                <p class="liste1">Certains appels HTTP
peuvent être secondés par des mécanismes
comme des queues de messages asynchrones. Ces approches permettent
de mieux répartir la charge sur les serveurs, mais également
dans le temps, en limitant à la capacité d’un
serveur le nombre d’ordres qui lui sont transmis sur une période
donnée (on parle de "thresholding", "threshold" signifiant
"seuil" en anglais).</p>
              </li>
              <li class="liste1">
                <p class="liste1">Les queues de messages peuvent éventuellement être
persistées sur disque dur et organisées&nbsp;de
façon qu’un message soit systématiquement acquitté par
le receveur avant son effacement, de façon à garantir
la livraison.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Afin de compléter les retry
policies, des circuits breakers peuvent être mis en place,
qui vont déconnecter un service fournissant des temps de
réponse inacceptables ou dégradant le niveau de
qualité d’autres services se basant sur lui. Il vaut mieux
en effet, si un service répond mais de manière
trop pénalisante, le débrancher complètement
et que le système trouve un autre moyen de réaliser
la tâche, soit simplement parce que d’autres instances
du même service sont disponibles et que le répartiteur
de charge va automatiquement diriger le trafic vers celles-ci, soit
parce que les appelants vont alors fonctionner de manière
dégradée mais en prenant leur responsabilité sur
le temps de réponse final.</p>
              </li>
              <li class="liste1">
                <p class="liste1">En fonction du besoin, les circuits
breakers peuvent être paramétrés pour continuer à lancer
des messages tests sur le service qui a été coupé,
puis décider de sa remise en ligne si le service satisfait à nouveau
aux exigences paramétrées.</p>
              </li>
            </ul>
          </div>
          <p class="defaut">Toutes ces fonctionnalités logicielles
peuvent paraître extrêmement contraignantes à mettre
en place, mais il existe désormais des outils open source
qui permettent de réaliser&nbsp;ces ajouts à peu
de frais. Surtout, certaines comme les retry policies sont indispensables&nbsp;au
fonctionnement en mode distribué, sous peine d’erreurs
difficilement possibles à identifier.</p>
        </div>
        <div class="sect3" id="refTitle8">
          <h3 class="title">d. Répartition de la charge</h3>
          <p class="defaut">Pour tirer pleinement parti des avantages
de Docker, il convient également de préparer le
logiciel pour que le passage à l’échelle (ajout
d’une instance de conteneur) soit effectivement synonyme d’augmentation
de performance. Pour caricaturer, si une architecture&nbsp;monolithique
traditionnelle est découpée sans réfléchir
en services Docker&nbsp;et qu’un conteneur est directement appelé par
un autre (par exemple un service de production de document PDF par
un service de mailing), le fait de rajouter une seconde instance
ne changera rien (les PDF ne seront pas générés
plus vite) car elle ne sera pas appelée par la première.
Il existe deux approches pour gérer ceci.</p>
          <p class="defaut">La plus logique et celle qui demande le moins
d’effort (voire aucun si l’orchestrateur l’implémente lui-même,
ce qui est courant) est de mettre en place un répartiteur
de charge et de faire pointer le service appelant sur une adresse qui
va répartir les messages entre les services. Ce mode de
fonctionnement est mis en pratique depuis longtemps, et il est même
tellement standard désormais que, comme vu dans les chapitres
précédents, il est maintenant intégré dans
le fonctionnement même de Docker qui va répartir
la charge automatiquement sur les différents conteneurs
démarrés sous le même service.</p>
          <p class="defaut">La seule modification du système éventuellement
nécessaire pour mettre en œuvre cette solution
est de modifier le paramètre correspondant à l’URL
du second service dans la configuration du premier. Dans une architecture
de services avec un bon niveau&nbsp;de maturité, c’est
un annuaire de service qui fournit cette URL, donc le premier service
n’a même pas à être reconfiguré.
Dans un cas un peu dégradé mais pleinement fonctionnel
où un simple proxy est utilisé (ce qui est tout à fait
acceptable dans la plupart des usages), le service n’a pas non plus à être
reconfiguré et ne nécessite donc pas plus d’être
redémarré. C’est le gros avantage de cette méthode
de répartition de la charge.</p>
          <p class="defaut">Vu que le load balancing est intégré dans
Docker Swarm et que nous l’avons utilisé dans les exemples
sur les chapitres précédents, nous ne nous appesantirons
pas plus sur le sujet, même s’il s’agit de la méthode
principale de répartition de charge.</p>
        </div>
        <div class="sect3" id="refTitle9">
          <h3 class="title">e. Inversion de consommation</h3>
          <p class="defaut">L’autre approche, qui nécessite éventuellement
de modifier le comportement de l’applicatif&nbsp;client, est
qu’il n’appelle plus directement le service dépendant, mais
qu’il émette une demande de traitement qui sera prise en
charge de manière asynchrone par un des services capables
de réaliser la tâche. Pour reprendre notre exemple,
le comportement serait le suivant&nbsp;:</p>
          <div class="divliste1">
            <ul class="liste1">
              <li class="liste1">
                <p class="liste1">Un service de mailing
publie - par exemple sur un flux RSS - les demandes de création
des fichiers PDF qu’il a pour mission d’envoyer aux clients de l’application.
Il réalise cette opération à son rythme,
potentiellement très rapide car il boucle peut-être
simplement sur une liste lue dans un fichier CSV et récupère
quelques données de clients dans une base de données
locale. En même temps qu’il publie ces demandes,&nbsp;le
service de mailing s’abonne à un événement
signalant qu’une demande&nbsp;donnée a été traitée.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Pendant ce temps, un ou plusieurs
services sont à l’écoute du flux RSS avec les
demandes&nbsp;à traiter. C’est d’ailleurs leur comportement
par défaut dès qu’ils sont démarrés.&nbsp;Ils
ne sont pas en attente passive qu’on les appelle sur une API&nbsp;;
au contraire, ils vont activement chercher du travail sur la liste
des tâches à réaliser par eux.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Quand ils en trouvent, ils déclarent
qu’ils prennent la tâche à leur compte. Comme
il y a de la compétition lorsque plusieurs services réalisent
la même opération, il est important que, lorsqu’un
service prend en charge une tâche, il le spécifie
tout de suite aux autres, de façon que ceux-ci ne réalisent
pas la même fusion de PDF.</p>
              </li>
              <li class="liste1">
                <p class="liste1">En règle générale,
le fait de ne pas attendre la fin de l’opération de fusion mais
de signaler dès qu’une tâche est prise en charge
suffit largement à ce qu’aucun autre service ne prenne
la tâche en même temps.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Dans les rares cas où deux
services réalisent la même tâche, ce
n’est souvent pas très grave fonctionnellement puisque
tous les services sont censés fournir un résultat équivalent.
Par contre, ce sera un gaspillage de ressources qui viendra pénaliser
l’augmentation de performance recherchée.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Lorsqu’un traitement en double peut
poser des problèmes fonctionnels, pour des ordres bancaires
par exemple, alors la réservation de tâche doit
se faire de manière transactionnelle, typiquement en gérant
une réservation en deux temps (premier temps, le service
prend la main et le système bloque les autres&nbsp;;
second temps, une fois les portes fermées, le système
vérifie que le premier service est bien seul sur cette
demande et lui donne la main).</p>
              </li>
              <li class="liste1">
                <p class="liste1">Dans tous les cas, un service est
maintenant en charge de la fusion du PDF. Avec les données
nécessaires, il réalise cette opération,
et lorsqu’elle est finie, il envoie un message comme quoi le résultat
est obtenu et à quel endroit il peut être récupéré par
le demandeur initial.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Le service de mailing reçoit
cette information (il s’y était lui-même abonné) et
récupère le fichier PDF. La demande avait déjà été marquée
comme réservée, elle peut être marquée
comme terminée, voire même supprimée
du flux RSS, puisqu’elle n’a plus aucune raison d’être
lue.</p>
              </li>
              <li class="liste1">
                <p class="liste1">L’ordre n’est pas nécessairement
respecté, car tous les services ne vont pas forcément à la
même vitesse, soit pour des raisons intrinsèques
(ils n’ont pas les mêmes quantités de ressources à disposition),
soit pour des raisons extrinsèques (ils ne sont pas sur
la même zone réseau, par exemple, et les appels à un
système de fichiers sont plus lents pour l’un que pour
l’autre). Si le service de mailing a une contrainte particulière
comme une règle métier exigeant que les courriels
partent exactement tous ensemble, c’est à lui de se débrouiller
pour stocker tous les PDF et les envoyer d’un seul coup. Dans le cas
plus logique où l’ordre n’a pas d’importance, le service
de mailing envoie directement le mail au client dès qu’il
a récupéré le PDF et associé le
bon texte personnalisé pour celui-ci (ceci nécessite
bien sûr une capacité de réconcilier
le retour avec la demande initiale, pour ne pas envoyer le PDF fusionné à l’attention
d’un client à un autre destinataire&nbsp;; la pratique
standard est de véhiculer dans les messages un identifiant
de corrélation).</p>
              </li>
            </ul>
          </div>
          <p class="defaut">L’approche décrite dans cette section
est bien sûr plus complexe à mettre en œuvre
que le load balancing, car elle nécessite de repenser le
service appelant ainsi que celui qui réalise l’opération
concernée si leur mode de fonctionnement normal était
celui (traditionnel) d’invocation de méthode décrit
plus haut. Les coûts de développement étant élevés,
cette réécriture peut représenter une
contrainte forte. Les avantages de cette approche sont toutefois
extrêmement intéressants, car non seulement les
problèmes d’erreurs suite à des surcharges sont
naturellement éliminés (les services de fusion&nbsp;du
PDF étant responsables de chercher du travail, ils travailleront
naturellement au rythme maximum soutenu, mais pas au-delà),
mais en plus la linéarité de la montée en
charge est bien meilleure. En particulier, l’ajout d’une instance
de service (par le démarrage d’un conteneur additionnel)
apporte naturellement un gain de performance, sans qu’il soit nécessaire
de changer quoi que ce soit dans une configuration de proxy (encore
que ce deuxième avantage soit mitigé par des outils
comme Traefik, qui automatisent la modification de configuration
et prennent en charge dynamiquement le nouveau conteneur dans le
reverse proxy).</p>
        </div>
        <div class="sect3" id="refTitle10">
          <h3 class="title">f. Gestion du multitenant au moyen des conteneurs<var style="display:none"> Multitenant</var></h3>
          <p class="defaut">Le multitenant est un pattern informatique
consistant à simuler plusieurs environnements étanches à partir
d’un seul processus informatique. Pour bien comprendre cette notion,
une comparaison peut être établie avec l’apport
de la virtualisation. Avant la virtualisation, un seul OS pouvait être
démarré en même temps sur une machine
physique. La virtualisation a permis de simuler plusieurs machines à partir
d’une seule, en garantissant que le comportement était
le même que si les machines avaient été effectivement
physiquement distinctes. L’économie était forte
car elle permettait d’acheter une seule machine.</p>
          <p class="defaut">Le multitenant suit le même principe,
mais au niveau logiciel. Traditionnellement, pour supporter un environnement
logiciel, une instance de processus est lancée, et si on
souhaite obtenir un autre environnement, il est nécessaire de
lancer un second processus. L’inconvénient&nbsp;est
que le coût en termes de ressources est assez fort, car
il est nécessaire de tout dupliquer en mémoire,
alors que beaucoup de fonctionnalités dans un logiciel
pourraient être partagées. Le principe du multitenant
est de simuler des applications&nbsp;différentes, étanches entre
elles, tout en n’utilisant en fait qu’un seul
processus applicatif. Un exemple simple est le lancement de plusieurs
documents dans un logiciel "multifenêtre"&nbsp;: une
seule instance du processus est lancée, mais les différents documents
ouverts le sont dans des fenêtres séparées
et l’utilisateur ne voit pas la différence avec un mode
où plusieurs instances applicatives similaires auraient été démarrées.&nbsp;L’effet
est surtout sur le système, car énormément
de ressources sont économisées au passage.</p>
          <p class="defaut">La comparaison avec les machines virtuelles
ne s’arrête pas là. Le caractère d’étanchéité est,
dans les deux cas, essentiel pour la robustesse de la solution, la
sécurité des données,&nbsp;etc. Côté virtualisation,
un manque même mineur d’étanchéité de
l’hyperviseur peut ouvrir une machine virtuelle à une attaque depuis
une autre. Pour un hébergeur fournissant des machines virtuelles,
il serait évidemment catastrophique qu’un client puisse
accéder d’une manière ou d’une autre aux données
d’un autre client. Le même problème existe sur
les architectures multitenants&nbsp;: même si un seul
processus informatique est utilisé, il est essentiel que
sa programmation soit exemplaire en termes de séparation des
environnements pour que le propriétaire d’un tenant ne
puisse jamais voir les données d’un autre tenant.</p>
          <p class="defaut">La gestion logicielle du multitenant est extrêmement
difficile à mettre au point, encore&nbsp;plus que la
virtualisation, car les ressources sont toutes partagées.
Un hyperviseur peut découper la mémoire en amont
du système d’exploitation, ce qui rend déjà les
choses plus simples. Et pourtant, même des hyperviseurs
avec dix ans de développement et plus par de grandes entreprises (elles
sont peu nombreuses à pouvoir financer des développements
aussi complexes) présentent encore parfois des failles
d’étanchéité.</p>
          <p class="defaut">Reproduire ce niveau de qualité dans
nombre d’applications logicielles, parfois avec des coûts
bien inférieurs, est clairement une gageure.</p>
          <p class="defaut">Docker est une alternative intéressante
au multitenant purement logiciel. En lançant plusieurs
conteneurs à partir d’une même image Docker, on
obtient des processus (un par conteneur) dont l’étanchéité est
assurée par les mécanismes du système
d’exploitation (technologie cgroups dans le cas de Linux). L’économie
de ressources n’est pas aussi forte que dans du multitenant logiciel,
mais elle est bien plus élevée que dans le cas
de la virtualisation, où le système d’exploitation
est complètement dupliqué d’un environnement à l’autre, ce
qui représente un immense gâchis de ressources.</p>
          <p class="defaut">Au final, Docker est une solution équilibrée
pour gérer du multitenant. Sa technologie assure une excellente étanchéité entre
deux clients si on les place sur deux conteneurs séparés,
et le coût en ressources reste mesuré. Docker
présente un autre avantage dans ce type d’approche, à savoir
que son API permet de lancer très facilement des conteneurs.
Ainsi, il est relativement simple de créer des ensembles
applicatifs où un tenant (donc un conteneur, dans ce cas) est
démarré lors de la souscription à un
service logiciel par un client (mode Software as a Service). De
plus, le démarrage d’un nouveau tenant est très
rapide (quelques secondes au maximum), surtout si on le compare
avec la création d’une machine virtuelle.</p>
        </div>
      </div>
      <div class="sect2" id="refTitle11">
        <h2 class="title">3. Gestion de la performance</h2>
        <p class="defaut">La performance est un critère essentiel
de fonctionnement correct d’un système d’information,&nbsp;et
l’introduction d’un nouveau venu dans des procédures d’audit
et de maintien de la performance bien rôdées peut
poser problème. Docker apporte des solutions&nbsp;aux
problèmes de performance, mais le mécanisme de
conteneurs ainsi que les architectures distribuées qui
lui sont associées nécessitent également
de repenser certains réflexes.</p>
        <div class="sect3" id="refTitle12">
          <h3 class="title">a. Passage à l’échelle automatique</h3>
          <p class="defaut">Le tout premier lien entre Docker et la performance,
immédiatement perçu par toute personne mettant
en œuvre des conteneurs, est la capacité à passer rapidement à l’échelle,
typiquement en utilisant la commande <span class="courier11">docker-compose
scale</span>&nbsp;; bref, de monter en charge comme expliqué un
peu plus haut.</p>
          <p class="defaut">Mais pour aller encore plus loin dans des
architectures modernes, il est possible d’automatiser&nbsp;ce
passage à l’échelle, en montant comme en descendant, grâce
par exemple à des règles métier sur la
charge CPU ou l’occupation mémoire. Ainsi, lorsque l’orchestrateur
(ou un autre logiciel) détecte qu’un service ne répond
pas suffisamment vite ou que les instances associées sont surchargées,
il peut décider de rajouter une instance de service et
répéter l’opération jusqu’à ce
que les métriques soient revenues à un état
normal.</p>
          <p class="defaut">La fréquence de cette opération
est bien sûr libre, et l’opération inverse peut être
mise en place pour ne pas risquer de surconsommation lorsque la
charge de travail redescend.</p>
          <p class="defaut">Cette gestion dite "autoscaling" est un sujet
autrefois complexe car les outils n’étaient pas librement
disponibles, mais Kubernetes propose désormais ce genre
de fonctionnalité, et il est également possible
de la mettre en œuvre avec Swarm, bien que moins directement
intégrée.</p>
        </div>
        <div class="sect3" id="refTitle13">
          <h3 class="title">b. Cas particulier du cache<var style="display:none"> Cache</var></h3>
          <p class="defaut">Le cache dont il est question ici n’est pas
le cache des images Docker sur une machine, qui permet de ne pas
télécharger une image depuis le registre alors que
ce n’est pas utile, mais le pattern de mise en cache des informations
dans une application. Les liens entre Docker et le cache sont très
faibles, mais comme pour la plupart des sections ci-dessus, il existe
de par le fait que Docker est massivement utilisé pour
mettre en œuvre des architectures distribuées. Or,
dans ce type d’architecture, n’importe quel conteneur doit pouvoir être supprimé et
redémarré à n’importe quel moment. Ce
qui veut dire que si le cache est associé au conteneur,
il est également perdu. Bien sûr, il se remontera ensuite,
mais cela prendra du temps pour revenir au même niveau
de performance.</p>
          <p class="defaut">De plus, si les services sont distribués
et que le cache est local, le risque est d’aboutir à une
forte duplication du cache sur les services dans le système. Bref,
et même si cela peut paraître pénalisant
du point de vue de la pure performance, il est important dans ce
type d’architecture d’éloigner le cache du service associé et
de le positionner dans un autre service, potentiellement distribué lui
aussi. Heureusement, c’est ce que supportent nativement des solutions
comme Redis ou Varnish.</p>
          <div class="note">
            <div class="remarkimg"><span class="icon-note"></span></div>
            <div class="divinline">
              <p class="remarque">En théorie, la mise en place
d’un cache n’est censée être qu’un pansement temporaire
sur un problème de performance, et qui permet "d’acheter
du temps" pendant que la résolution en profondeur du problème
est réalisée. En pratique, les caches ne sont
presque jamais remis en question, ce qui peut poser des problèmes
lors de l’évolution des applications, car les goulets d’étranglement
peuvent se déplacer et les caches finir par causer plus
de problèmes qu’ils n’en résolvent. Dans le cas
d’architectures distribuées, la maîtrise du cache
est à la fois complexifiée par la multiplicité des
acteurs et facilitée par la spécialisation des
requêtes (pourvu bien sûr que l’alignement entre
les fonctionnalités métier et la technique soit
bon).</p>
            </div>
          </div>
        </div>
        <div class="sect3" id="refTitle14">
          <h3 class="title">c. Alignement sur le CPU et les threads</h3>
          <p class="defaut">Dans certains cas particuliers de serveurs
web comme Node.js, un seul thread est utilisé,&nbsp;avec
une boucle d’événements pour gérer les
temps d’attente et traiter toutes les commandes de manière
asynchrone. Ceci permet d’améliorer grandement les performances
en supprimant les temps d’attente sur des opérations longues
comme les entrées/sorties fichiers ou les accès
réseau. Cependant, il est rare que les applications utilisant
ce mode de fonctionnement tirent également parti des multiples
processeurs à disposition, car elles ne sont en général
pas multithread.</p>
          <p class="defaut">Dans le cas de Node.js, il existe des moyens
de mettre en place plusieurs threads de traitement dans un seul
serveur web de type Nginx, en utilisant Phusion Passenger. Dans
un mode Docker, c’est alors l’ensemble qui sera mis en place dans
une image, et cette image sera instanciée généralement
sans limite de consommation de CPU.</p>
          <p class="defaut">Une autre approche consiste à créer
une image basée uniquement sur un Node.js et à instancier
cette image dans de multiples conteneurs Docker pour gérer
la montée en charge. Il est alors possible de spécifier
que chaque conteneur ne doit utiliser qu’un seul CPU, vu que de
toute façon le processus qu’il embarque ne sera pas capable
d’en exploiter plus d’un.</p>
          <p class="defaut">Le choix d’une approche ou d’une autre répond à de
nombreux critères liés à la façon
de compiler les applications, à la manière dont
elles utilisent les ressources, etc., et il est impossible d’en
recommander, du strict point de vue de la performance, une plutôt
qu’une autre. Seules des mesures de performance sur des scénarios
métier permettront effectivement de dire quelle est la
meilleure solution dans votre cas.</p>
          <div class="note">
            <div class="remarkimg"><span class="icon-note"></span></div>
            <div class="divinline">
              <p class="remarque">Du point de vue de l’élégance
de l’architecture, toutefois, la seconde approche permet de simplifier
la gestion logicielle en supprimant un serveur applicatif (et ses
configurations). Un autre avantage est le meilleur alignement de
l’applicatif sur la fonctionnalité, ce qui respecte les
recommandations de Docker et facilite la montée en charge
(en théorie bien sûr, seuls des benchmarks pouvant
mesurer les gains en pratique).</p>
            </div>
          </div>
        </div>
        <div class="sect3" id="refTitle15">
          <h3 class="title">d. Approche "production only"</h3>
          <p class="defaut">Une approche qui commence à trouver
de l’écho dans les milieux industriels et les conférences
d’experts informatiques est que la gestion de multiples environnements
est un surcoût qu’il convient de diminuer. Pour prendre
un exemple, un environnement de préproduction est souvent
mis en place pour recetter une application logicielle dans des conditions
proches de la production. Dans le meilleur des cas, les machines
seront similaires en nombre et en qualité.</p>
          <p class="defaut">Parfois, l’effort est poussé jusqu’à reproduire
des conditions de charge de l’environnement avec de réels
logs de requêtes repoussés dans le nouvel environnement.
Pourtant, chaque administrateur a connu des bugs en production. Non
seulement il est impossible d’assurer qu’une préproduction
captera tous les problèmes en amont, mais en plus certaines
différences empêchent que ce soit le cas (niveau
de remplissage des différents caches, impossibilité de
reproduire certains appels coûteux, comme des API payantes,
etc.).</p>
          <p class="defaut">Le foisonnement des environnements entre recette,
développement, test, intégration, préproduction,
production (et même parfois jusqu’à des prérecette, intégration
système, etc.) pose également problème.
La ressource informatique a beau ne pas être chère
quand on peut la consommer temporairement au lieu de l’acquérir,
le coût de gestion associé peut rapidement augmenter
et les impacts environnementaux sont également à prendre
en compte.</p>
          <p class="defaut">Partant de ce principe, certains architectes
ont émis comme principe que, quitte à devoir&nbsp;surveiller
attentivement la production et mettre en place des mesures de remise&nbsp;en
condition opérationnelle, autant se servir de ce même environnement
pour recetter les modifications. Le raisonnement derrière cette
approche (qui peut paraître risquée mais limite
en fait le risque sous certaines conditions) est le suivant&nbsp;:</p>
          <div class="divliste1">
            <ul class="liste1">
              <li class="liste1">
                <p class="liste1">Des résolutions
de bugs en production sont inévitables, donc autant les
voir le plus tôt possible et surtout au moment où tous
les yeux sont rivés dessus pour les traiter au plus vite.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Le surcoût d’un environnement
supplémentaire sans garantie aucune d’absence de bugs sur
la production n’est pas justifiable.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Seul l’environnement de production
permet de voir ce que voient nos utilisateurs. Il faut donc tester
nos nouvelles fonctionnalités sur cet environnement pour être
proche de leur expérience.</p>
              </li>
              <li class="liste1">
                <p class="liste1">De nombreuses méthodes
existent pour mitiger le risque de tester en production&nbsp;:</p>
              </li>
            </ul>
          </div>
          <div class="divliste2">
            <ul class="liste2">
              <li class="liste2">
                <p class="liste2">Il existe des outils
permettant d’assurer une forte étanchéité des
processus et une capacité à les démarrer
et les arrêter très rapidement, par exemple Docker.</p>
              </li>
              <li class="liste2">
                <p class="liste2">Une architecture réalisée
dans un mode de consommation inversée des services permet
d’assurer de manière logicielle que l’ajout d’un service
ne pourra pas avoir d’impact sur les fonctionnalités existantes
s’il ne fait qu’observer des événements et y réagir à l’intérieur
de son conteneur.</p>
              </li>
              <li class="liste2">
                <p class="liste2">Dans les cas simples, il existe
des fenêtres de tir dans les environnements où il
est possible d’expérimenter sans dommage pour l’énorme
majorité des utilisateurs. Par exemple, personne n’utilise
une application de déclaration des impôts deux
mois après la date de fermeture des déclarations ou
trois mois avant l’ouverture de la nouvelle campagne. Et il existe
encore de nombreuses applications liées à une
consommation locale, où une interruption nocturne ne pose
pas de problème insurmontable.</p>
              </li>
              <li class="liste2">
                <p class="liste2">Dans les cas plus complexes, des
solutions logicielles permettent de mettre en place une fonctionnalité de
façon qu’elle ne soit visible que par certains utilisateurs.
Dans un premier temps, ce peut être uniquement les testeurs
internes de l’application, puis, si tout va bien, les beta testeurs chez
les clients, puis un pourcentage réduit des clients, puis
un plus grand nombre d’entre eux, puis au final tout le monde si
aucune erreur n’est survenue.</p>
              </li>
            </ul>
          </div>
          <p class="defaut">Bien que ce point ne soit pas directement
lié à la performance, il est important de garder
en tête que revenir sur des habitudes comme la multiplicité des
environnements peut être un facteur à la fois
de réduction de l’utilisation des ressources, mais aussi
de prise en compte effective de l’expérience utilisateur,
et donc de la perception de la performance par les acteurs les plus
importants du système d’information.</p>
          <div class="note">
            <div class="remarkimg"><span class="icon-note"></span></div>
            <div class="divinline">
              <p class="remarque">Eric Brechner (l’auteur précise
qu’il est le traducteur en français de son livre "Hard
Code") a développé cet aspect de la gestion des
environnements depuis 2010 dans son article de blog fondateur intitulé "There’s
no place like production". Cet article peut être retrouvé en
ligne sur le site des blogs MSDN (<a class="url" href="https://blogs.msdn.microsoft.com/eric_brechner/2010/11/30/theres-no-place-like-production/" target="_blank">https://blogs.msdn.microsoft.com/eric_brechner/2010/11/30/theres-no-place-like-production/</a>).</p>
            </div>
          </div>
        </div>
      </div>
      <div class="sect2" id="refTitle16">
        <h2 class="title">4. Sécurité<var style="display:none"> Sécurité</var></h2>
        <div class="sect3" id="refTitle17">
          <h3 class="title">a. Mangue ou noix de coco&nbsp;?</h3>
          <p class="defaut">Docker est indéniablement apte à la
production dans les environnements les plus sensibles, où les
architectures cloisonnées qu’il permet sont même
un atout pour la sécurité en profondeur des systèmes
(principe de Defence in Depth, ou défense en rideaux).</p>
          <p class="defaut">Là où une application monolithique
rendra visible son contenu complet lors de la découverte&nbsp;de
la moindre faille comme une injection SQL, une application utilisant
correctement des microservices sera beaucoup plus résistante aux
attaques, et ce pour plusieurs raisons&nbsp;:</p>
          <div class="divliste1">
            <ul class="liste1">
              <li class="liste1">
                <p class="liste1">L’accès à la
base de données, et donc à la valeur principale
de l’application, ne peut se faire qu’en ayant trouvé une
faille sur le microservice. Or, ceux-ci sont multiples et il y a
de bonnes chances qu’un microservice exposé ne soit pas
directement en contact avec la base de données. Il faudra
donc trouver une faille dans le premier microservice, mais aussi
dans le second et ainsi de suite pour arriver à la base
ou à des applications sensibles comme les gestions financières.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Pour compliquer les choses, il n’est
pas impossible que ces multiples microservices utilisent des technologies
différentes, ce qui fait qu’une faille trouvée
sur l’un ne fonctionnera pas sur le suivant, et qu’il faudra donc
produire un effort d’attaque démultiplié.&nbsp;</p>
              </li>
              <li class="liste1">
                <p class="liste1">Comme, de plus, les bases de données
peuvent être réparties, l’accès à l’une d’entre
elles ne permettra de voir qu’une partie de la donnée.
Pour peu qu’elle soit obfusquée ou inutilisable quand on
ne dispose pas des données issues d’une autre base, le
pirate sera encore au point mort.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Enfin, tout ceci n’est que de la
défense passive, et un administrateur efficace aura tôt
fait de découper les couches de microservices sur des réseaux
différenciés, rendant une attaque encore plus
complexe car elle devra également déjouer les
contremesures réseau et les blocages supplémentaires
ainsi mis en place.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Pour finir, le fonctionnement des
microservices de frontend en mode immutable, avec des conteneurs
qui sont recyclés très rapidement et se basent
sur une image en lecture seule, achèvera de renforcer l’architecture
avec une sécurité contre le défaçage,
qu’il est extrêmement complexe de contourner.</p>
              </li>
            </ul>
          </div>
          <p class="defaut">Ces approches sont qualifiées de
"mangue plutôt que noix de coco", la première étant
molle en surface, mais de plus en plus dure au fur et à mesure qu’on
s’approche du noyau (ce qui est plutôt bon pour une architecture
de sécurité) alors que la seconde est plus dure à première
vue, mais perd tout son jus une fois un simple trou percé dans
sa coque (métaphore de la faille d’injection SQL qui permet
de contourner toutes les sécurités entre une page
web et une base de données à cause d’un défaut
de programmation qu’un pirate aguerri saura découvrir et
exploiter).</p>
        </div>
        <div class="sect3" id="refTitle18">
          <h3 class="title">b. Évolution de la gestion de la sécurité par
Docker</h3>
          <p class="defaut">Le précédent livre sur Docker
(du même auteur, publié aux Éditions
ENI) concluait en décrivant tous les problèmes
de sécurité qu’il restait à Docker à gérer
pour atteindre la maturité attendue. Il se trouve que tous
les points qui posaient à l’époque problème
ont été corrigés, conformément
aux engagements de Docker Inc.</p>
          <p class="defaut">Longtemps, Docker a été en
retard sur la sécurité. L’entreprise
a certainement voulu occuper le marché le plus vite possible
pour ne pas céder un pouce à la concurrence, et
ce au prix parfois de la sécurisation des installations.
Cette époque est heureusement révolue depuis que
le logiciel lui-même est devenu stable et que du temps
a été consacré à sa sécurisation.</p>
          <p class="defaut">Dans les dernières technologies apparues
sur 2016 et 2017, la sécurité est désormais
pensée dès l’apparition des nouvelles
fonctionnalités. C’est le cas en particulier de
la mise en place d’un cluster Docker Swarm, où le
rattachement d’un démon au cluster ne se fait
que s’il connaît un jeton secret. Le jeton est par
ailleurs dédoublé entre une version pour se rattacher
en tant que simple agent d’exécution ou bien pour
rejoindre le cluster en tant que nœud de type manager.</p>
          <p class="defaut">De bonnes pratiques apparaissent sur la sécurisation
des images, avec l’utilisation de la commande <span class="courier11">USER</span> pour les <span class="courier11">Dockerfile</span> ou
l’option <span class="courier11">--user</span> dans un <span class="courier11">docker run</span>, de façon à ne
pas fonctionner en <span class="courier11">root</span>. SELinux
et AppArmor sont désormais pleinement pris en compte. Bref,
le sujet est maintenant correctement traité, et nous ne
nous lancerons pas dans une section (nécessairement très
longue) pour expliquer tous les mécanismes de sécurité de
Docker. La consultation de la page <a class="url" href="https://docs.docker.com/engine/security/" target="_blank">https://docs.docker.com/engine/security/</a> donnera aux
plus experts toutes les pistes nécessaires.</p>
          <p class="defaut">Plus récemment, le fait que les réseaux
soient attachables par défaut (c’est-à-dire qu’un
conteneur pouvait s’y rattacher selon sa propre volonté)
a été modifié pour améliorer
la sécurité des déploiements en cluster.</p>
          <p class="defaut">Bref, non seulement le manque de protection
du Docker Engine a été comblé, mais les
nouvelles fonctionnalités sont désormais livrées
dès le début dans des versions correctement sécurisées
(par exemple, la technologie Docker Swarm inclut la sécurisation
des échanges entre démons Docker par chiffrement
cryptographique). La course à la fonctionnalité caractéristique
des années de jeunesse de beaucoup de technologies cherchant à occuper
le marché est donc bel et bien terminée pour Docker,
sauf évidemment rechute à surveiller.</p>
          <div class="note">
            <div class="remarkimg"><span class="icon-note"></span></div>
            <div class="divinline">
              <p class="remarque">Sur ces défauts de jeunesse
de Docker (mais aussi ses qualités), l’auteur se permet
de renvoyer à un de ses articles, publié dans
Le Mag IT&nbsp;: <a class="url" href="http://www.lemagit.fr/tribune/Jean-Philippe-Gouigoux-auteur-aux-Editions-ENI-Docker-est-un-jeune-homme-a-la-tete-bien-faite" target="_blank">http://www.lemagit.fr/tribune/Jean-Philippe-Gouigoux-auteur-aux-Editions-ENI-Docker-est-un-jeune-homme-a-la-tete-bien-faite</a></p>
            </div>
          </div>
          <p class="defaut">En plus de cela, Docker a livré dans
les dernières années des fonctionnalités
ciblées spécifiquement sur la sécurité, épargnant
donc du travail à l’administrateur exploitant un cluster
Docker et qui devait auparavant prendre en charge ces aspects de
la sécurisation. On peut citer par exemple deux technologies
qui vont dans ce sens&nbsp;:</p>
          <div class="divliste1">
            <ul class="liste1">
              <li class="liste1">
                <p class="liste1">Docker Secrets&nbsp;:
il s’agit de fonctionnalités permettant la gestion d’un
ensemble de données centralisées par Docker et
qui peut être échangé entre participants à un
cluster Swarm de manière sécurisée à la
fois pour le transit et pour l’accès. Ainsi, seuls les
conteneurs désignés auront accès à ces
données secrètes (mots de passe, certificats,
etc.), mais leur échange sur le réseau sera en
plus réalisé sous un format chiffré.
Cet ensemble de données est associé au cycle de
vie du Swarm, et a donc une garantie de survie aussi forte que le
cluster lui-même.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Docker Notary&nbsp;: il s’agit
de fonctionnalités permettant de signer des contenus et
de vérifier leur authenticité et leur origine
dans le cadre de Docker. Ces contenus sont bien entendu en priorité des
images, et Docker Notary permettra de valider qu’une image téléchargée
auprès d’un éditeur a effectivement été publiée
par cet éditeur et qu’il ne s’agit pas d’une contrefaçon
posant, par exemple, un problème de sécurité.</p>
              </li>
            </ul>
          </div>
          <div class="note">
            <div class="remarkimg"><span class="icon-note"></span></div>
            <div class="divinline">
              <p class="remarque">Ces deux technologies peuvent être
retrouvées en détail respectivement sur les pages
de documentation <a class="url" href="https://docs.docker.com/engine/swarm/secrets/" target="_blank">https://docs.docker.com/engine/swarm/secrets/</a> et <a class="url" href="https://docs.docker.com/notary/getting_started/" target="_blank">https://docs.docker.com/notary/getting_started/</a></p>
            </div>
          </div>
          <p class="defaut">Un excellent signe de la maturité de
Docker sur la sécurité est qu’il commence à apparaître
des guides de sécurisation qui ne sont plus de simples
listes compilées par des experts, mais de vrais documents
formalisés et porteurs d’une réelle approche méthodologique.
Très récemment, par exemple, l’ANSSI (Agence nationale
pour la sécurité des systèmes d’information)
a publié une fiche technique intitulée "Recommandations
de sécurité relatives aux déploiements
de conteneur Docker" (<a class="url" href="https://www.ssi.gouv.fr/uploads/2020/10/docker_fiche_technique.pdf" target="_blank">https://www.ssi.gouv.fr/uploads/2020/10/docker_fiche_technique.pdf</a>).
Voici quelques-unes des recommandations issues de la fiche&nbsp;:</p>
          <div class="divliste1">
            <ul class="liste1">
              <li class="liste1">
                <p class="liste1">Utiliser l’option <span class="courier11">--bridge=none</span> pour éviter
que les conteneurs puissent par défaut&nbsp;se voir
sur le réseau bridge (mesure de cloisonnement).</p>
              </li>
              <li class="liste1">
                <p class="liste1">Réduire les capabilities
(privilèges) utilisés par défaut pour
les conteneurs à ceux uniquement nécessaires.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Restreindre systématiquement
les ressources (mémoire, CPU, espace de stockage) lors
du lancement des conteneurs.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Journaliser les conteneurs.</p>
              </li>
            </ul>
          </div>
        </div>
        <div class="sect3" id="refTitle19">
          <h3 class="title">c. Pratique de sécurisation des ports</h3>
          <p class="defaut">Une bonne pratique, comme dans n’importe quel
cas autre que Docker, est de n’exposer que les ports qui seront
effectivement utiles, de façon à limiter la surface
d’attaque d’un applicatif.</p>
          <p class="defaut">Il est toutefois possible de renforcer encore
cette sécurité en jouant sur le fait que les <span class="courier11">iptables</span> sont des ressources sur lesquelles
un conteneur Docker peut jouer de manière&nbsp;étanche
par rapport à celles de son hôte Docker (comme pour
le filesystem, les numéros de processus, etc.). Pour cela,
il est nécessaire d’utiliser l’option <span class="courier11">--cap-add=NET_ADMIN</span> lors
de la commande <span class="courier11">docker run</span>. Une fois
cette manipulation réalisée, <span class="courier11">iptables</span> peut être
installé dans le <span class="courier11">Dockerfile</span> et
de nouvelles règles peuvent être mises en place
qui n’auront d’impact que dans le contexte du conteneur. Ceci est
particulièrement pratique pour gérer la sécurité au
niveau applicatif, alors que c’est une activité très
complexe lorsque les applications sont toutes installées
sur la même machine.</p>
          <div class="note">
            <div class="remarkimg"><span class="icon-note"></span></div>
            <div class="divinline">
              <p class="remarque">Bien sûr, avec la capacité récente
de Docker de créer des réseaux overlay séparés
par application, et même plusieurs réseaux séparés à l’intérieur
du même Docker Compose, assurant ainsi une excellente séparation
des services, cette fonctionnalité n’est plus aussi importante
qu’auparavant.</p>
            </div>
          </div>
        </div>
        <div class="sect3" id="refTitle20">
          <h3 class="title">d. Sécurité sur l’utilisation des images</h3>
          <p class="defaut">Une section complète avait été consacrée
dans le précédent livre sur Docker aux Éditions
ENI sur la sécurité liée au choix des
images, avec en particulier la recommandation d’utiliser au mieux
des images officielles, et au pire de valider correctement les <span class="courier11">Dockerfile</span> et d’utiliser les images
compilées de manière sécurisée
par Docker Hub pour s’assurer de leur correspondance avec le <span class="courier11">Dockerfile</span>. Nous ne reviendrons pas
sur ces bonnes pratiques, mais préciserons quelques bonnes
pratiques additionnelles sur les numéros de version, la
gestion de la distribution et l’utilisation d’outils additionnels.</p>
          <p class="defaut">Afin de bénéficier des toutes
dernières mises à jour de sécurité sur
une image officielle, il est recommandé de bien prendre
les dernières images disponibles (étiquette <span class="courier11">latest</span>), même si c’est opposé à la
recommandation de fixer une version&nbsp;: le choix est alors
entre la stabilité de l’application et sa sécurisation, les
deux n’étant pas possibles à avoir en même
temps, car la sécurisation nécessite une réactivité aux
failles constatées, et donc des mises à jour.
Dans la pratique, les versions patchées sont en général
parfaitement compatibles avec les versions précédentes.
L’idéal est donc de rester sur des versions d’images de base
avec un numéro fixe, mais de faire évoluer et
de recompiler ces images dès qu’une version nouvelle est
disponible. C’est évidemment du travail en plus, mais si
le coût paraît important, il faut toujours le
comparer avec celui d’un piratage éventuel et de la probabilité de
celui-ci&nbsp;; bref, réaliser une analyse de risques.</p>
          <p class="defaut">Comme cela a été constaté dans
le chapitre sur le déploiement logiciel, les images Docker&nbsp;issues
d’un registre sont désormais systématiquement
accompagnées d’un condensat qui permet de valider leur
téléchargement correct et la non-modification
par un processus malicieux lors de ce téléchargement
(typiquement, par une attaque de type Man In The Middle). Encore
une fois, il s’agit d’un problème de sécurité de
jeunesse qui a été corrigé par Docker
Inc. conformément à ses engagements.</p>
          <p class="defaut">La commande <span class="courier11">docker
pull</span> utilise cette marque pour valider le téléchargement
correct des images. Le registre privé Docker proposé en
standard dans l’écosystème reste un peu en retrait,
souffrant de manques sur la gestion des droits et de la complexité de
se brancher à des outils standards d’authentification.
Il existe toutefois des alternatives (utiliser un registre en mode
SaaS comme Azure Container Registry que nous avons exposé dans
un précédent chapitre, démultiplier les
registres, installer un Nginx en reverse proxy avec authentification,
etc.).</p>
          <p class="defaut">Docker Inc. a travaillé avec le Center
for Internet Security pour obtenir un benchmark sur Docker Engine
avec toutes les recommandations et bonnes pratiques de sécurité liées à Docker.
En plus de cet énorme document, des développeurs
(internes à Docker Inc. comme externes) ont eu la bonne
idée de produire en open source un outil nommé Docker
Bench for Security qui peut être utilisé pour
détecter des non-conformités dans des images par
rapport à ces recommandations.</p>
          <p class="defaut">L’outil est disponible sur <a class="url" href="https://github.com/docker/docker-bench-security" target="_blank">https://github.com/docker/docker-bench-security</a> et son
exécution permet d’obtenir des rapports très détaillés
sur les points à sécuriser dans vos images Docker.
Ses recommandations vont d’utiliser la version de Docker à jour à restreindre
l’utilisation CPU sur les conteneurs, en passant par ne pas utiliser
la valeur <span class="courier11">always</span> pour le paramètre <span class="courier11">on-failure</span>, etc. Bien sûr,
les règles sont assorties d’une sévérité qui
permet de juger ce qui doit obligatoirement être corrigé de
ce qui peut attendre&nbsp;une prochaine version.</p>
          <div class="note">
            <div class="remarkimg"><span class="icon-note"></span></div>
            <div class="divinline">
              <p class="remarque">Des outils comme XRay de JFrog permettent également
d’explorer les images Docker et de retrouver les vulnérabilités
connues des composants logiciels embarqués, ce qui est
un excellent complément au précédent
outil.</p>
            </div>
          </div>
        </div>
        <div class="sect3" id="refTitle21">
          <h3 class="title">e. Restriction sur les ressources</h3>
          <p class="defaut">Un autre point essentiel avant de démarrer
une production sur une architecture de conteneurs est de bien prendre
en compte l’équilibrage des ressources, qui nécessite
la fixation de quotas. Dans des architectures à microservices,
les conteneurs assurent une grande étanchéité entre
services (voire entre tenants),&nbsp;et cette étanchéité ne
peut être complète que si elle concerne aussi les
ressources.</p>
          <p class="defaut">Docker permet de limiter l’usage mémoire
d’un conteneur, grâce à l’option <span class="courier11">--memory</span>, ainsi que son usage du processeur, à l’aide
de l’option <span class="courier11">--cpushares</span>. Sans entrer
dans les détails, une commande comme celle-ci permet de
limiter les deux ressources&nbsp;:</p>
          <pre class="programlisting"><code class="hljs sql">docker run -it <span class="hljs-comment">--memory 256m --cpushares 256 ubuntu</span></code></pre>
          <p class="defaut">La mémoire est limitée en
taille absolue, tandis que la limite sur les tranches de processeur
est exprimée de manière relative. Tant que le
conteneur exemple ci-dessus est seul sur la machine hôte,
il peut utiliser 100&nbsp;% du CPU (dans la limite
de ce que lui fournit le noyau Linux, bien sûr). Mais si
un second conteneur démarre avec une valeur&nbsp;de <span class="courier11">--cpushares</span> de 512, par exemple, le
premier conteneur devra alors se contenter d’un tiers des tranches
disponibles tandis que le second en obtiendra deux tiers. À l’inverse,
si la valeur choisie pour le second conteneur est de 128, le rapport
sera inversé.</p>
        </div>
      </div>
    </div></div></div></div></app-page-content><!----><!----><!----></div></kendo-pdf-export>