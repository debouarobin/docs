<kendo-pdf-export _ngcontent-uvf-c168="" papersize="A4" margin="2cm" id="div-book-content" class="col-12 col-sm-10 offset-0 div-book-content" uipath_custom_id="10"><div><app-page-content _ngcontent-uvf-c168="" _nghost-uvf-c166="" class="font-size-14 font-size-sm-20"><div _ngcontent-uvf-c166="" class="mainContent mt-4"><div _ngcontent-uvf-c166="" id="bookContent" class="bookContent"><div _ngcontent-uvf-c166="" id="content"><div class="sect1" style="abcpdf-tag-visible: true" id="AU_088f49a5-a853-4690-aeb0-c5ee91d47c08" uri="ImagesUri_../download/5c180f28-f752-47db-98e8-8656c84b148d/images/">
      <h1 class="title">Le besoin d’orchestration<var style="display:none"> Orchestration</var></h1>
      <div class="sect2" id="refTitle0">
        <h2 class="title">1. Objectif</h2>
        <p class="defaut">Jusqu’à maintenant, tous nos déploiements
ont été réalisés en ciblant
la machine locale&nbsp;comme hôte des conteneurs. Lorsque
nous parlions déploiement, le registre permettait&nbsp;de
mettre à disposition de machines extérieures des images
créées sur une machine donnée, mais il
n’empêche que ces machines extérieures, après
s’être connectées au registre pour récupérer
l’image, instanciaient les conteneurs correspondants de manière
locale. Le fonctionnement restait donc unitaire, et finalement pas
très différent d’un déploiement sur la machine
ayant servi à créer l’image. </p>
        <p class="defaut">Dans le présent chapitre, nous allons étudier
les moyens de découper cette logique et de séparer
la machine pilotant le déploiement de la ou des machines qui
vont porter effectivement les conteneurs. Le but principal de ce
découpage est de pouvoir cibler facilement plusieurs machines
pour la seconde partie, et ainsi de pouvoir facilement étendre
sa capacité de déploiement de conteneurs, en ajoutant
des ressources matérielles de manière transparente
par rapport au déploiement. Bref, le but du clustering
sur les conteneurs est de permettre le déploiement de conteneurs
sur un ensemble de machines qui se comportent comme une seule et énorme
entité hôte de conteneurs.</p>
        <p class="defaut">Un autre but qui est fortement corrélé est
de déployer un nombre variable d’instances de certains
conteneurs, de façon à équilibrer au
mieux la charge de ces conteneurs. Après tout, c’est une
des raisons principales pour laquelle nous avions décomposé une
application en microservices&nbsp;: pour pouvoir facilement jouer
sur le nombre d’instances et les ressources allouées à chaque
service, afin d’atteindre un équilibre entre ces services
qui soit synonyme d’utilisation optimale des ressources disponibles.</p>
        <p class="defaut">Pour mettre en relation les machines d’un
cluster pour qu’elles s’échangent des charges de travail,
pour répartir correctement les conteneurs d’une application,
pour gérer les échanges réseau nécessaires
et pour assurer le fonctionnement en continu de l’ensemble&nbsp;même
en cas de panne locale, il faut une plateforme qu’on dit "d’orchestration".
Après une explication théorique des enjeux du
mode distribué, deux orchestrateurs seront présentés&nbsp;:
un très simple et efficace, très intégré à Docker, à savoir
Swarm&nbsp;; un beaucoup plus sophistiqué et qui est à ce
jour l’état de l’art, répondant au nom de Kubernetes.</p>
        <p class="defaut">Bien que Swarm soit un bon produit et une
excellente façon de mettre en place un premier niveau d’orchestrateur,
l’orientation actuelle de l’informatique est de ne faire de la place
que pour un seul "gagnant", et clairement, Kubernetes est le choix
logique pour une mise en place d’infrastructure distribuée.
La question s’est posée de montrer le fonctionnement de
Swarm dans le présent ouvrage, mais au lieu de le mettre
au second&nbsp;plan, c’est même l’inverse qui a été décidé, à savoir
de détailler Swarm et de ne faire que présenter
rapidement Kubernetes, et ceci pour plusieurs raisons.<var style="display:none"> Kubernetes</var></p>
        <p class="defaut">Tout d’abord, pour des raisons de place, il était
plus logique de détailler Swarm, ce qui ne prend que quelques
dizaines de pages, car cet orchestrateur est très simple à mettre
en œuvre. Kubernetes, pour sa part, fait l’objet de deux livres
aux mêmes Éditions ENI, dont un introductif, auquel
l’auteur a participé, et un autre beaucoup plus approfondi.
Swarm correspond à un périmètre trop
restreint pour bénéficier d’un livre dédié,&nbsp;mais
en même temps, il serait dommage de l’abandonner. Comme
en plus il s’agit d’une technologie fournie en même temps
que Docker lui-même, il était logique de traiter
de Swarm dans le présent ouvrage.<var style="display:none"> Swarm</var></p>
        <p class="defaut">De plus, expliquer des concepts complexes
comme l’orchestration des conteneurs sur une technologie sophistiquée
et de mise en œuvre lourde comme Kubernetes rend le sujet
encore plus difficile à traiter. La légèreté de
Swarm et sa très grande économie de moyens sont
des alliés précieux pour expliquer les principes
sans qu’ils se retrouvent cachés derrière des
détails techniques peu opportuns. Pour une approche introductive,
Swarm est donc plus indiqué&nbsp;: le seul fait de
commencer les explications de l’orchestration&nbsp;par un mécanisme assez
simple est intéressant en termes pédagogiques,
car cela permet d’aborder des concepts relativement complexes de
nœuds, clusters et distribution de charges dont la compréhension
préalable permettra de s’attaquer&nbsp;plus efficacement à Kubernetes,
dont la maîtrise est éminemment plus difficile.&nbsp;</p>
        <p class="defaut">Enfin, et c’est le pari de l’auteur, il n’est
pas impossible que Swarm ait été mis de côté un
peu trop rapidement par un domaine informatique qui, en tant que royaume
de l’abstraction, a souvent tendance à qualifier d’idéales
les solutions les plus sophistiquées et à traiter
de manière condescendante des solutions plus simples et
pragmatiques.</p>
        <p class="defaut">À lire les nombreux retours d’expérience
se plaignant de la complexité de Kubernetes et de la difficulté de
trouver des experts, on peut légitimement se poser des
questions sur l’adéquation de Kubernetes dans des déploiements limités.
Alors que ce système est justement censé s’occuper
de tout ce qui est complexe, ce besoin de spécialistes
pour son maintien en condition opérationnelle peut être
perçu comme paradoxal. Les articles sur le sujet finissent
en général par conseiller de ne pas opérer
soi-même une infrastructure Kubernetes, mais plutôt
de la laisser gérer par un Cloud. Cette approche&nbsp;n’est
pourtant pas toujours la meilleure, et surtout, cela n’a pas de
sens qu’elle soit imposée par une solution qui est censée
simplifier fortement la gestion d’un cluster de machines.</p>
        <p class="defaut">En fait, il existe de très nombreuses
situations où Kubernetes est clairement surdimensionné par
rapport au besoin, et Swarm est alors une alternative simple, efficace
et qui, de plus, n’est pas incompatible à un passage à Kubernetes
dans un second temps, pour le cas où la complexité croissante
des déploiements le justifierait.</p>
        <div class="note">
          <div class="remarkimg"><span class="icon-note"></span></div>
          <div class="divinline">
            <p class="remarque">Au-delà de la simple question
de la taille du cluster, les questions à se poser pour
départager&nbsp;Swarm ou Kubernetes portent sur le
besoin d’un ingress sophistiqué, sur le degré de
robustesse souhaité, etc. Les namespaces de Kubernetes
permettent une gestion avancée des groupes de conteneurs,
mais dans certains cas, le seul fait de créer plusieurs
clusters Swarm séparés peut tout à fait
faire l’affaire. C’est aussi la raison de ce chapitre que de donner
suffisamment de compréhension des deux plateformes pour
que le lecteur fasse un choix éclairé, le meilleur
orchestrateur dépendant bien sûr du contexte.</p>
          </div>
        </div>
      </div>
      <div class="sect2" id="refTitle1">
        <h2 class="title">2. Approche théorique</h2>
        <div class="sect3" id="refTitle2">
          <h3 class="title">a. La problématique de montée en charge</h3>
          <p class="defaut">Un problème auquel est confronté presque
tout administrateur d’une application web au cours de sa vie est
l’augmentation forte des ressources consommées par une
application. Des optimisations logicielles ou d’organisation peuvent
parfois être réalisées, mais globalement,
la ressource utilisée croît avec le nombre d’utilisateurs
et les fonctionnalités exposées.</p>
          <p class="defaut">Le schéma ci-dessous montre les différentes étapes
par lesquelles un administrateur "traditionnel" peut théoriquement
passer&nbsp;:</p>
          <div class="image">
            <div class="mediaobject"><img class="imagedata picturebox" alt="images/06EP01.png" title="images/06EP01.png" src="IMAGES/06EP01.png?id=AAEAAAD%2f%2f%2f%2f%2fAQAAAAAAAAAMAgAAAE1FbmkuRWRpdGlvbnMuTWVkaWFwbHVzLCBWZXJzaW9uPTEuMC4wLjAsIEN1bHR1cmU9bmV1dHJhbCwgUHVibGljS2V5VG9rZW49bnVsbAUBAAAAJ0VuaS5FZGl0aW9ucy5NZWRpYXBsdXMuQ29tbW9uLldhdGVybWFyawIAAAAHcGlzVGV4dAlwaWR0ZURhdGUBAA0CAAAABgMAAAAyTGF2b2NvIEVuem8gLSBiNGFhNGUzYy01ODBkLTQyNWQtOWMwOC1mNDBjMjRlNTY0ZmMdRFquy7jYiAs%3d"></div>
          </div>
          <div class="divliste1">
            <ul class="liste1">
              <li class="liste1">
                <p class="liste1">L’étape 1
est la mise en production pour un ensemble d’utilisateurs restreints, à savoir&nbsp;généralement
les seules personnes connaissant l’application avant qu’elle ne
soit globalement répertoriée sur Internet&nbsp;:
typiquement, les testeurs et personnes intéressées
de l’entreprise publiant le site ou l’application. À ce
niveau de charge, tout se passe généralement bien.
Chaque module de l’application (des conteneurs Docker, par exemple)
est à l’aise avec la part de ressources qu’il peut consommer.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Imaginons que l’application fonctionne
correctement et que les utilisateurs sont au rendez-vous. La croissance
aboutira à l’étape 2, à savoir que les conteneurs
augmentant leur besoin en ressources, ils occuperont au final toute
la puissance de la machine&nbsp;hôte.</p>
              </li>
              <li class="liste1">
                <p class="liste1">À ce moment-là,
un facteur limitant apparaîtra sur la mémoire
vive, le stockage, la bande passante réseau ou la puissance
de calcul (voire même sur plusieurs de ces caractéristiques).&nbsp;Une
des approches les plus simples pour éviter des pertes de
performance - ou même des réponses en erreur dans
le pire de cas - &nbsp;est de réaliser l’opération
nommée "scale up" et qui consiste à augmenter
la taille de la machine utilisée. C’est ce que nous représentons à l’étape
3. Si le système n’est pas virtualisé, ceci nécessite&nbsp;de
réinstaller la totalité des briques logicielles,
et si l’application n’a pas été structurée
pour gérer cette éventualité, le coût
peut être élevé.</p>
              </li>
              <li class="liste1">
                <p class="liste1">En règle générale,
l’administrateur prévoyant qui a été confronté à ce
besoin de "scale up" prend ses précautions et, si l’application
est prévue pour augmenter encore&nbsp;son empreinte,
il choisira une machine suffisamment bien dotée pour voir
venir les prochains pics d’exploitation. Pourtant, quelles que soient
les ressources financières&nbsp;à disposition,
il arrive un moment où il n’est plus possible d’acheter
une machine plus grosse (en pratique, la limite où l’application
n’est plus en mesure d’exploiter correctement des ressources plus
abondantes peut parfois être atteinte avant, typiquement
lorsqu’une application ne peut tirer parti des processeurs multiples).
C’est l’étape 4 qui représente ce moment dans
la croissance théorique d’une application.<var style="display:none"> Scale up</var></p>
              </li>
              <li class="liste1">
                <p class="liste1">Le coût a augmenté de
manière forte sur ces dernières étapes,
mais la complexité est toutefois contenue. Le passage à l’étape
5, obligatoire car le "scale up" ne peut plus fonctionner, va permettre
de réduire drastiquement les coûts, mais au prix
d’une complexité en hausse&nbsp;: il consiste à passer
au "scale out", c’est-à-dire à l’augmentation
du nombre de machines. Dans un premier temps, comme précédemment,
l’approche&nbsp;est relativement simple&nbsp;: il suffit de
mettre en place deux machines et de répartir intelligemment
les conteneurs entre celles-ci, en fonction de la connaissance que
l’administrateur a de leur fonctionnement (connaissance qui nécessite
une communication avec les développeurs ayant créé ladite
application, ce qui est parfois en soi un problème).</p>
              </li>
              <li class="liste1">
                <p class="liste1">Ensuite, le "scale out" va, lui
aussi, connaître sa phase de complexification, lorsqu’il
va falloir gérer plusieurs instances d’un même
service pour continuer à augmenter en performance. Pour
cela, il faudra introduire dans le système un mécanisme
de répartition&nbsp;de charge (load balancing en anglais), comme
HAProxy par exemple. L’étape 6 montre ce cas de figure,
avec deux instances du conteneur A, et le symbole du load balancer
au-dessus des machines logiques.<var style="display:none"> Scale out</var><var style="display:none"> Répartition de charge</var><var style="display:none"> Load balancing</var></p>
              </li>
              <li class="liste1">
                <p class="liste1">Si la croissance se poursuit, de
plus en plus de machines seront nécessaires pour gérer&nbsp;les
multiples instances de chacun des services (comme montré à l’étape
7 ci-dessus). De plus, si l’habitude a été prise
de gérer des gros conteneurs occupant toute la ressource
d’un serveur logique, le nombre de ces derniers va commencer à poser
problème.</p>
              </li>
              <li class="liste1">
                <p class="liste1">La solution proposée à ce
problème serait de disposer d’un système qui,
bien que composé de multiples machines logiques (physiques
ou virtuelles), apparaîtrait comme un seul hôte
Docker, ce qui permettrait une gestion plus simple, tout en rendant
possible la présence de nombreux conteneurs. C’est cette
solution, schématisée par l’étape 8,
que nous allons étudier plus avant dans ce chapitre.</p>
              </li>
              <li class="liste1">
                <p class="liste1">Le principal avantage de cette solution
est que la mise à l’échelle étant complètement
disjointe entre le nombre de machines ou leur taille et le nombre de
conteneurs que l’ensemble va pouvoir porter, il est très
simple de faire évoluer indépendamment ces deux
caractéristiques. À l’augmentation des performances
attendues&nbsp;correspondra l’augmentation du nombre de conteneurs
Docker. Ceci se traduira par la nécessité d’augmenter
la taille de l’hôte Docker "virtuel", ce qui se fera par
ajout de machines dans ce qu’on appelle communément le
cluster (grappe en français, mais ce vocabulaire est très peu
utilisé). C’est l’étape 9, d’agrandissement du
parallélépipède représentant
un cluster, qui illustre ceci.</p>
              </li>
            </ul>
          </div>
        </div>
        <div class="sect3" id="refTitle3">
          <h3 class="title">b. La solution découplée</h3>
          <p class="defaut">Revenons un peu plus en détail sur
la solution proposée en étape 8 ci-dessus. Elle
consiste à séparer (on parle parfois de "découplage")
la taille et le nombre des machines physiques et virtuelles supportant
le démon Docker du dimensionnement de l’application utilisant
celui-ci. Pour arriver à cette séparation des
deux notions, il est nécessaire qu’un client Docker soit
capable d’adresser un ensemble d’hôtes Docker de la même
manière qu’il appellerait le démon positionné sur
une machine hôte seule. Pour cela, il convient que le mode
d’appel du démon ne soit pas lié strictement à des
caractéristiques d’une machine. L’utilisation du protocole
TCP pour échanger avec les démons&nbsp;permet
précisément ceci, car seul un identifiant de type
IP est nécessaire pour spécifier au client Docker
quel démon ou quel ensemble de démons (en passant
par une des machines) il doit contacter. C’est ce qui est réalisable
en modifiant la valeur de la variable d’environnement <span class="courier11">DOCKER_HOST</span>&nbsp;:</p>
          <pre class="programlisting"><code class="hljs typescript"><span class="hljs-keyword">set</span> DOCKER_HOST = tcp:<span class="hljs-comment">//172.99.79.150:2376</span></code></pre>
          <p class="defaut">Le fait que le contrat entre un client Docker
et un serveur Docker soit aussi simple est une première
partie de la solution. La seconde tient dans le fait qu’un ensemble
de démons&nbsp;Docker en réseau peut être
adressé par un seul couple adresse IP + port réseau,&nbsp;correspondant à n’importe
quel des nœuds de gestion du cluster (le vocable anglais&nbsp;de
"manager node" est souvent utilisé). Ce nœud de
gestion se charge ensuite de pousser les bonnes instructions Docker
sur les nœuds de travail (on parle de "worker&nbsp;nodes").
Enfin, la troisième partie de la solution tient dans le
fait que l’API Docker est contractuelle, c’est-à-dire qu’elle
reste la même qu’on accède à un démon
ou à un réseau de démons. Ces trois caractéristiques
aboutissent ensemble au découplage&nbsp;précité entre
la taille du cluster et le nombre de démons Docker vus
par le client, à savoir systématiquement un.</p>
          <div class="note">
            <div class="remarkimg"><span class="icon-note"></span></div>
            <div class="divinline">
              <p class="remarque">L’API Docker est standardisée
par le biais de deux spécifications de l’Open Container
Initiative, un consortium web ouvert de normalisation de la gestion des
conteneurs auquel&nbsp;adhèrent Docker ainsi que de
nombreuses autres grandes sociétés d’informatique.
La première de ces spécifications concerne la
gestion de l’exécution des conteneurs (et donc l’API pour
les démarrer, les arrêter, etc.) et la seconde
la définition des images. Plus d’informations sur <a class="url" href="https://www.opencontainers.org/" target="_blank">https://www.opencontainers.org/</a></p>
            </div>
          </div>
        </div>
        <div class="sect3" id="refTitle4">
          <h3 class="title">c. Conséquences sur l’approche initiale</h3>
          <p class="defaut">Nous avons montré l’existence d’un
mode "élastique" où les conteneurs n’ont pas à se
poser la question de la structure des machines virtuelles ou physiques sous-jacentes
au démon Docker qui les accueillent. Dès lors,
la question se pose de directement créer les infrastructures
de déploiement dans ce mode, et ce sans attendre que la
montée en charge ne nous y contraigne.</p>
          <p class="defaut">Plusieurs arguments vont dans ce sens. Tout
d’abord, le fait que le cluster puisse n’être composé que
d’une machine permet de limiter au maximum les coûts de
fonctionnement en première étape. Ce n’est certes
pas le cas pour la part d’investissement de ces coûts,
mais nous verrons un peu plus loin qu’elle est très limitée
depuis les dernières versions de Docker, l’installation
d’un cluster étant à peine plus complexe que l’installation
d’un démon Docker sur une seule machine. Elle peut même être
limitée encore plus fortement en faisant appel à un
fournisseur dédié, qui proposera ce service en
ligne (on parle alors de CaaS, pour <span class="italic">Container as a Service</span>).<var style="display:none"> Container as a Service</var></p>
          <p class="defaut">De plus, le coût des changements
de machines et d’infrastructures tels que montrés en théorie
dans les étapes décrites plus haut est en général
très élevé, non seulement pour le matériel
mais aussi à cause de la mise en œuvre, potentiellement
complexe. Et ceci sans même compter les coûts
d’image à cause des possibles ruptures de services dans
ces moments à risque pour la production.</p>
          <p class="defaut">À l’inverse, il ne faut pas négliger
le coût de formation à cette nouvelle façon de
gérer les serveurs, qui va à l’encontre de ce
que bon nombre d’administrateurs ont mis en pratique pendant des
années. Dans le mode traditionnel, l’administrateur connaît
chacun des serveurs par son nom, il sait ce qui se trouve dessus,
a l’habitude de gérer telle ou telle spécificité.
Lorsque le serveur montre des signes de fatigue, sa connaissance
de ce serveur particulier lui permet de rapidement déterminer
la criticité du problème et l’urgence à le
régler par rapport à d’autres. Dans ce nouveau
mode où les machines ne sont que des supports d’un cluster,
l’administrateur les gère de manière indifférenciée.
Il en va d’ailleurs de même pour les conteneurs, puisqu’il
ne peut pas savoir, dans un cluster, sur quelle machine quels conteneurs
vont s’exécuter. Le système peut même
arrêter dynamiquement un conteneur sur un serveur en même
temps qu’il le redémarre sur un autre.</p>
          <p class="defaut">Bref, l’administrateur n’a plus du tout le
même travail et ne supervise pas un cluster Docker de la
même manière qu’il supervise des serveurs logiques
ou physiques.</p>
          <p class="defaut">Le changement cité ci-dessus est
désigné par l’expression "cattle versus pet"&nbsp;: littéralement,
l’administrateur gère désormais du bétail
plutôt que des animaux de compagnie. Au lieu de connaître
chaque serveur et chaque conteneur par son nom et de savoir ce qu’il
fait précisément, comment, avec quelle spécificité (comme
avec des animaux domestiques, qu’on nomme et qu’on individualise),
l’administrateur doit changer de point de vue et se comporter comme un éleveur
qui gère un troupeau de bétail&nbsp;: il ne
connait pas chaque tête individuellement, et certaines
bêtes peuvent mourir pendant que d’autres naissent, mais
globalement il sait que la taille est à peu près
la bonne et s’arrange pour mener le troupeau d’un point à un
autre sans embûches.</p>
          <div class="note">
            <div class="remarkimg"><span class="icon-note"></span></div>
            <div class="divinline">
              <p class="remarque">L’image véhiculée
par cette approche "bétail plutôt qu’animal domestique" peut
porter à sourire, mais il s’agit d’une pratique désormais
bien ancrée et qui représente sans conteste l’état
de l’art actuel de la gestion des infrastructures de déploiement
applicatifs en ligne.</p>
            </div>
          </div>
        </div>
      </div>
      <div class="sect2" id="refTitle5">
        <h2 class="title">3. Lien aux microservices<var style="display:none"> Microservices</var></h2>
        <div class="sect3" id="refTitle6">
          <h3 class="title">a. Orchestration des services<var style="display:none"> Orchestration</var></h3>
          <p class="defaut">Le chapitre précédent s’est
terminé sur une section relative à l’orchestration fonctionnelle
des microservices. Lorsque des services doivent travailler ensemble,
nous avons vu qu’il est possible de mettre en œuvre une
orchestration des API, ou parfois de manière&nbsp;plus
simple des relations évènementielles constituant
une chorégraphie de services. L’orchestration dont on parle
dans ce cas est bien celle des services logiciels alignés
sur les fonctionnalités, et elle correspond (en résumant à l’extrême)
aux enchaînements&nbsp;d’activités et de tâches
qui constituent les processus métier au sens Business Process
Model.<var style="display:none"> API</var></p>
          <p class="defaut">Lorsque nous utilisons le mot orchestration
dans le présent chapitre, il s’agit d’un tout autre concept,
bien que des points communs se retrouvent naturellement entre deux
approches utilisant le même vocabulaire. L’orchestration des
conteneurs Docker consiste à répartir ces conteneurs
sur un ensemble de machines qui travaillent de concert dans un cluster,
et à faire en sorte que ces conteneurs échangent
correctement entre eux bien qu’ils se trouvent sur des machines
différentes.</p>
        </div>
        <div class="sect3" id="refTitle7">
          <h3 class="title">b. Élasticité</h3>
          <p class="defaut">L’orchestration recouvre aussi le fait de
relancer des conteneurs, de démultiplier les instances
de conteneurs pour implémenter un service en mode load balancing,
etc. La toute première raison de mettre en place un cluster
est en effet souvent le besoin de dépasser les limites
physiques d’une seule machine en mettant en réseau plusieurs
machines&nbsp;tout en faisant en sorte que le démon
Docker apparaisse comme unique.</p>
          <p class="defaut">Or, une fois une machine ajoutée,
pourquoi ne pas en ajouter d’autres en fonction des besoins&nbsp;?
Comme les conteneurs ne voient que le démon Docker, leur montée
en nombre est globalement décorrélée
de l’augmentation des ressources machines sous-jacentes, même
si les secondes doivent toujours être suffisantes pour
la première. Cette capacité d’ajouter facilement
des ressources sans impacter l’application elle-même, et
donc de s’adapter dynamiquement à la charge, est qualifiée
d’élasticité. C’est une des caractéristiques principales
des services de cloud et des architectures distribuées,
qui a eu un impact majeur sur les modes de fonctionnement et a permis
l’émergence d’approches "as a service", de paiement à la
consommation, de mise à disposition de ressources virtuellement
illimitées en échange d’un abonnement, etc.</p>
        </div>
        <div class="sect3" id="refTitle8">
          <h3 class="title">c. Robustesse</h3>
          <p class="defaut">Enfin, les orchestrateurs ont un rôle
dans le maintien en condition opérationnelle des charges
applicatives portées sur le cluster. En effet, orchestrer
les conteneurs en les répartissant&nbsp;sur une infrastructure élastique
est une chose, mais maintenir ensuite ce fonctionnement de manière
dynamique y compris lorsqu’un nœud du cluster tombe en
panne ou lorsqu’un événement réseau empêche
les machines de communiquer en est une autre. Or, plus le cluster est
gros et plus ce type d’événement a des chances
de se produire.</p>
          <p class="defaut">Pour compliquer le tout, les charges applicatives évoluent
en permanence avec des versions nouvelles, des patchs à déployer,
des évolutions du paramétrage et de la configuration,
des déploiements et des niveaux de sécurisation évoluant
et des enjeux de traçage et de sécurisation des échanges
web à de nombreux niveaux.</p>
          <p class="defaut">Seul un logiciel dédié a
la capacité de gérer toutes ces fonctionnalités
de manière transparente pour un conteneur, de façon
qu’il puisse se concentrer sur la qualité de son processus
applicatif dans sa boîte noire sans avoir à se
soucier de l’environnement dans lequel il est lancé, qu’il
s’agisse d’une simple machine isolée ou d’un énorme
cluster hybride&nbsp;porté par plusieurs clouds distants.
Il en va du bon partage des responsabilités et de la portabilité des
conteneurs.</p>
        </div>
      </div>
      <div class="sect2" id="refTitle9">
        <h2 class="title">4. Fonctionnement pratique</h2>
        <div class="sect3" id="refTitle10">
          <h3 class="title">a. Notion de réseau<var style="display:none"> Réseau</var></h3>
          <p class="defaut">Nous avons jusqu’ici utilisé indifféremment
les dénominations "cluster" (grappe, en français)
et "réseau". Une grappe désigne habituellement
un ensemble de ressources de même type, qui sont agrégées
par une caractéristique externe. Par exemple, une grappe
de serveurs est un ensemble de machines, souvent localisées
au même endroit de façon à travailler
de concert. La notion de réseau est plus forte car elle
porte l’idée de communication entre toutes les ressources.
Dans un réseau Ethernet, par exemple, toutes les machines
appartenant à un réseau peuvent se parler les
unes aux autres, et elles partagent des protocoles et des conventions
qui font qu’elles ne sont pas qu’un groupe de machines physiquement
mises en commun.</p>
          <p class="defaut">Pour continuer sur la comparaison avec le
réseau au sens informatique, cette capacité à communiquer
entre les différentes entités le constituant forme
ce qu’on appelle un maillage, c’est-à-dire une grille de
relations liant les "nœuds" du réseau (les entités
y appartenant) entre eux.</p>
          <p class="defaut">La notion de réseau de démons
Docker suit exactement le même principe. Chaque démon&nbsp;est
un nœud du réseau et il a la capacité de
parler avec les autres nœuds, le tout formant un maillage
de démons Docker qui peuvent s’échanger des messages.
Dans le cas d’un réseau Docker, les messages portent en
particulier sur la capacité à déployer
ce qu’on appelle des services. Nous reviendrons plus loin en détail
sur cette notion, mais dans un premier temps, nous pouvons réaliser
une approximation grossière avec les conteneurs qui sont
exécutés sur les démons. Ainsi, les démons
parlent entre eux pour se donner la possibilité de répartir
un ensemble de conteneurs à exécuter de manière&nbsp;équilibrée
sur le réseau qu’ils constituent. On parle alors d’orchestration,
et là aussi, nous reviendrons avec maints détails
sur cette notion.</p>
          <p class="defaut">Bien évidemment, tout réseau
nécessite un support "physique" pour s’exprimer&nbsp;: à la
fin des fins, pour que le maillage puisse être réalisé,
il faut que des liens existent entre les nœuds. Dans le
cas d’un réseau informatique, c’est la couche physique
(le fil de cuivre ou bien l’onde dans le cas du Wifi) qui porte le
reste des couches. De la même manière, un réseau
de démons Docker s’appuie sur le réseau informatique
reliant les machines entre elles, et en particulier sur le protocole
TCP.</p>
        </div>
        <div class="sect3" id="refTitle11">
          <h3 class="title">b. Les différents types de nœuds<var style="display:none"> Nœuds</var></h3>
          <p class="defaut">Nous avons évoqué un peu
plus haut le principe des nœuds de type "gestionnaire"
(manager en anglais) et des nœuds de travail (worker nodes
en anglais). On parle parfois aussi pour ces derniers d’agents.
Il convient d’expliquer les caractéristiques détaillées
des deux afin de faciliter la compréhension des opérations
de mise en place qui sont décrites ci-après.</p>
          <p class="defaut">Les agents sont des nœuds de pure
exécution&nbsp;: leur travail consiste à prendre en
charge des conteneurs, mais ce ne sont pas eux qui décident
de quels conteneurs ni des conditions de démarrage ou de
mise à l’échelle.</p>
          <p class="defaut">Les nœuds gestionnaires sont ceux
qui centralisent cette orchestration et qui vont justement décider
quel conteneur est démarré sur quel démon,
avec quelle méthode de répartition du travail,
etc. Bref, ils sont la partie intelligente du réseau. Toutefois,
par défaut, un nœud gestionnaire est également
un agent, c’est-à-dire qu’il est en mesure de faire fonctionner
des conteneurs comme les autres. Ce choix fait sens, vu que la charge
d’orchestration est assez faible et que dédier un nœud
uniquement à cette tâche serait un gaspillage
de ressources. Dans des contextes particuliers, il est toutefois
possible de réserver toute la puissance du nœud
gestionnaire à la distribution des tâches et de
paramétrer l’orchestrateur de façon que le gestionnaire
ne supporte aucune&nbsp;tâche, toutes étant
alors assignées aux agents. Le nœud gestionnaire n’a
alors besoin que du client Docker pour réaliser son office
de pilote (on parle aussi de Docker CLI, pour <span class="italic">Command Line Interface</span>, interface
en ligne de commande en français).</p>
          <p class="defaut">Attention, bien qu’il soit courant pour des
infrastructures réduites ou de test (comme nous en mettrons
en place dans le présent chapitre) de n’utiliser qu’un
seul nœud gestionnaire pour le réseau, il est
de bonne pratique en production de tripler, voire quintupler, ce
rôle de gestionnaire, afin d’éviter qu’il constitue
un point unique d’échec de l’ensemble (on parle de SPOF,
pour <span class="italic">Single Point Of Failure</span>).
Les contraintes de robustesse dictent d’avoir plus d’un gestionnaire,
mais ce n’est pas pour autant que tous les gestionnaires travaillent de
concert. En pratique, un gestionnaire est désigné comme
actif par un processus d’élection entre les différents
gestionnaires et c’est lui qui s’occupera&nbsp;de la distribution
des tâches sur les agents. S’il vient à tomber
en panne, les autres gestionnaires détecteront ce changement
de situation et un processus d’élection viendra faire en
sorte qu’un nouveau gestionnaire prenne la main.</p>
        </div>
        <div class="sect3" id="refTitle12">
          <h3 class="title">c. Fonctionnalités du cluster</h3>
          <p class="defaut">Un cluster est un système distribué,
c’est-à-dire qu’il n’y a pas de point central. Il est également
dit déclaratif, car nul ne sait à l’avance quel
manager va prendre la main sur la gestion du cluster, et nul ne
sait quel nœud de travail sera utilisé pour exécuter
telle instance de service. Les applications et les services sont
simplement déclarés et l’orchestrateur&nbsp;se
chargera de les répartir sur les machines au mieux, selon
ses algorithmes, tout en prenant bien sûr en compte les
préférences de configuration ainsi qu’une logique
de meilleure utilisation des ressources.</p>
          <p class="defaut">De plus, cette logique d’optimisation est
constante dans le temps. Si un serveur physique ou virtuel sous-jacent
ne répond par exemple plus, le cluster va se charger par
son rôle d’orchestrateur de redistribuer les services sur
les machines restantes pour équilibrer au mieux la charge
du cluster.</p>
        </div>
      </div>
    </div></div></div></div></app-page-content><!----><!----><!----></div></kendo-pdf-export>